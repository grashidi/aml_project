{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import  DataLoader\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from covid_dataset import CovidDataset\n",
    "from train_util import fit, test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\n",
    "    BATCH_SIZE = 10 # adpated from paper\n",
    "    NUM_EPOCHS = 10 # adpated from paper\n",
    "    USE_CACHE = True\n",
    "\n",
    "    root_dir = [\"../../data/ct_scan/\", \"../../data/xray/\"]\n",
    "    txt_COVID = \"data_split/COVID/\"\n",
    "    txt_NonCOVID = \"data_split/NonCOVID/\"\n",
    "\n",
    "    trainset = CovidDataset(root_dir=root_dir,\n",
    "                            txt_COVID=txt_COVID + \"/train.txt\",\n",
    "                            txt_NonCOVID=txt_NonCOVID + \"/train.txt\",\n",
    "                            train=True,\n",
    "                            use_cache=USE_CACHE)\n",
    "    valset = CovidDataset(root_dir=root_dir,\n",
    "                          txt_COVID=txt_COVID + \"/val.txt\",\n",
    "                          txt_NonCOVID=txt_NonCOVID + \"/val.txt\",\n",
    "                          use_cache=USE_CACHE)\n",
    "    testset = CovidDataset(root_dir=root_dir,\n",
    "                           txt_COVID=txt_COVID + \"/test.txt\",\n",
    "                           txt_NonCOVID=txt_NonCOVID + \"/test.txt\",\n",
    "                           use_cache=USE_CACHE)\n",
    "\n",
    "    train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, drop_last=False, shuffle=True)\n",
    "    val_loader = DataLoader(valset, batch_size=BATCH_SIZE, drop_last=False, shuffle=True)\n",
    "    test_loader = DataLoader(testset, batch_size=BATCH_SIZE, drop_last=False, shuffle=True)\n",
    "\n",
    "    # # check out some of the images\n",
    "    # for batch_index, batch_samples in enumerate(train_loader):\n",
    "    #     im, labels = batch_samples['img'], batch_samples['label']\n",
    "    #     plt.imshow(im[0,1,:,:].numpy(), alpha=1.0)\n",
    "    #     plt.savefig(\"test_\" + str(batch_index) + \".png\")\n",
    "    #\n",
    "    #     if batch_index > 18:\n",
    "    #         break\n",
    "\n",
    "    # # compute mean and std for dataset\n",
    "    # mean = 0.\n",
    "    # std = 0.\n",
    "    # for batch_samples in train_loader:\n",
    "    #     images, labels = batch_samples['img'], batch_samples['label']\n",
    "    #     samples = images.size(0) # batch size (the last batch can have smaller size!)\n",
    "    #     images = images.view(samples, images.size(1), -1)\n",
    "    #     mean += images.mean(2).sum(0)\n",
    "    #     std += images.std(2).sum(0)\n",
    "    #\n",
    "    # mean /= len(train_loader.dataset)\n",
    "    # std /= len(train_loader.dataset)\n",
    "    #\n",
    "    # print(mean, std)\n",
    "\n",
    "\n",
    "    # load model pretrained on ImageNet\n",
    "    densenet121 = models.densenet121(pretrained=True)\n",
    "\n",
    "    # replace fully connected layer\n",
    "    densenet121.fc = nn.Linear(in_features=512, out_features=2, bias=True)\n",
    "\n",
    "    # freeze all layers\n",
    "    for param in densenet121.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # unfreeze last three layers\n",
    "    for layer in [densenet121.layer4, densenet121.avgpool, densenet121.fc]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    #train ...\n",
    "    time = datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\")\n",
    "    stats_path = \"model_backup/stats_densenet121_e{}_bs{}_{}.pt\".format(NUM_EPOCHS,\n",
    "                                                                     BATCH_SIZE,\n",
    "                                                                     time)\n",
    "\n",
    "    optimizer = optim.Adam(densenet121.parameters())\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    fit(densenet121, optimizer, scheduler, criterion, train_loader, val_loader, NUM_EPOCHS, stats_path)\n",
    "    test(densenet121, criterion, test_loader)\n",
    "\n",
    "    if not os.path.exists(\"model_backup/\"):\n",
    "        os.makedirs(\"model_backup/\")\n",
    "\n",
    "    torch.save(densenet121.state_dict(),\n",
    "               \"model_backup/densenet121_e{}_bs{}_{}.pt\".format(NUM_EPOCHS,\n",
    "                                                             BATCH_SIZE,\n",
    "                                                             time))\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "19a1ed34ad81e3b6c0cce9835184af7b1fabfe02c49492686006d87779355413"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}